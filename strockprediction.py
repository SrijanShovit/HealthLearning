# -*- coding: utf-8 -*-
"""StrockPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/123TGpy2sBgkNnJMQgwwwInR3BfLh6JTz

### Predicting Stroke Risk: A Data-Driven Approach

Stroke is a leading cause of death and disability worldwide, making early prediction and prevention crucial. Leveraging machine learning, we can analyze key health indicators and lifestyle factors to assess the risk of stroke. This project utilizes a comprehensive dataset from Kaggle, including variables such as age, hypertension, heart disease, and lifestyle habits, to build a predictive model. By accurately identifying individuals at high risk, we aim to contribute to public health awareness and preventive healthcare efforts.

---

### Steps for Predicting Stroke Risk

1. **Import Libraries**
   - Load necessary Python libraries for data manipulation, machine learning, and evaluation.

2. **Load the Data**
   - Read the dataset into a pandas DataFrame for analysis and preprocessing.

3. **Handle Missing Values**
   - Identify and fill or remove any missing values in the dataset to ensure data integrity.

4. **Encode Categorical Variables**
   - Convert categorical features into numerical values using techniques like One-Hot Encoding to make them suitable for machine learning algorithms.

5. **Split the Data into Features and Target**
   - Separate the dataset into independent variables (features) and the dependent variable (target) for model training.

6. **Split the Data into Training and Testing Sets**
   - Divide the data into training and testing subsets to evaluate the model's performance on unseen data.

7. **Feature Scaling**
   - Standardize the feature values to have a mean of zero and a standard deviation of one to improve the model's performance.

8. **Train the Model**
   - Use a machine learning algorithm, such as Random Forest, to train a predictive model on the training data.

9. **Make Predictions**
   - Apply the trained model to the test data to predict the target variable.

10. **Evaluate the Model**
    - Assess the model's performance using metrics like accuracy, confusion matrix, and classification report to determine its effectiveness.
"""

# Step 1: Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/healthcare-dataset-stroke-data.csv')

data.head()

data.tail()

# Check for missing values
print(data.isnull().sum())
#columns bmi contains some null values

# Fill missing values in the 'bmi' column with the median
data['bmi'].fillna(data['bmi'].median(), inplace=True)

data.head()

# Check for missing values
print(data.isnull().sum())
#Now we remove all the null values from our data

# Convert categorical columns to numerical (One-Hot Encoding)
data = pd.get_dummies(data, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], drop_first=True)

# Feature Importance Plot
importances = model.feature_importances_
feature_names = X.columns

# Create a DataFrame for feature importances
feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importances')
plt.show()

# Assuming 'stroke' is the target variable
X = data.drop(['id', 'stroke'], axis=1)  # Drop 'id' as it's not a feature
y = data['stroke']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize the Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)
print(y_pred)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(class_report)

#Visualization

# Confusion Matrix Heatmap
plt.figure(figsize=(8, 6))
conf_matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['No Stroke', 'Stroke'], yticklabels=['No Stroke', 'Stroke'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

#by ManishRana | contributor
#manish_61873_40534